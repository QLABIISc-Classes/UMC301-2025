{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41ce62a8-251f-4f9e-b375-e93a5861c3fe",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c949cf",
   "metadata": {},
   "source": [
    "## Why RAG?\n",
    "\n",
    "### 1. Knowledge Cutoff\n",
    "*They don't know information after their training date*\n",
    "\n",
    "- **Training data freeze**: Models trained on data up to specific cutoff date.\n",
    "- **Growing knowledge gap**: Information becomes increasingly outdated as time passes\n",
    "- **Missing recent events**: No knowledge of current news, updates, releases, or discoveries\n",
    "- **Example**: Can't answer \"Who won the 2024 election?\" or \"Latest iPhone features\"\n",
    "\n",
    "### 2. No Real-time Data\n",
    "*Can't access current information*\n",
    "\n",
    "- **Static knowledge only**: Cannot connect to internet, APIs, or live databases\n",
    "- **No dynamic information**: Cannot fetch stock prices, weather, traffic, or breaking news\n",
    "- **Frozen in time**: Information reflects training period, not current state\n",
    "- **Example**: Cannot provide today's weather or current market conditions\n",
    "\n",
    "### 3. Hallucination\n",
    "*May generate plausible but incorrect information*\n",
    "\n",
    "- **Confident fabrication**: Creates believable but false information when uncertain\n",
    "- **Pattern-based guessing**: Fills knowledge gaps with plausible-sounding responses\n",
    "- **No verification mechanism**: Cannot fact-check or validate generated content\n",
    "- **Example**: May invent fake statistics, URLs, or medical advice\n",
    "\n",
    "### 4. Domain-Specific Knowledge\n",
    "*Limited knowledge about your private/company data*\n",
    "\n",
    "- **Public data only**: Only knows information available during training\n",
    "- **No private access**: Cannot read internal documents, policies, or proprietary data\n",
    "- **Generic responses**: Cannot provide company-specific procedures or information\n",
    "- **Example**: Cannot answer questions about internal APIs, company policies, or customer data\n",
    "\n",
    "### 5. Memory Limitations\n",
    "*Can't remember previous conversations*\n",
    "\n",
    "- **No conversation history**: Each session starts fresh with no memory of past interactions\n",
    "- **Context window limits**: Forgets earlier parts of long conversations\n",
    "- **No user preferences**: Cannot learn or adapt to individual user needs over time\n",
    "- **Example**: User must re-explain context and preferences in every new session\n",
    "\n",
    "## How RAG Solves These Problems\n",
    "\n",
    "RAG bridges these gaps by:\n",
    "- **Retrieving current information** from updated knowledge bases\n",
    "- **Grounding responses** in verified, sourced content\n",
    "- **Accessing private data** through custom document collections\n",
    "- **Maintaining context** through conversation and document history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94a43a7",
   "metadata": {},
   "source": [
    "## Part 0: Introduction, Installations and Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a87870",
   "metadata": {},
   "source": [
    "**Indexing**\n",
    "\n",
    "1. Load: First we need to load our data. This is done with [Document Loaders](https://python.langchain.com/docs/concepts/document_loaders/).\n",
    "2. Split: [Text splitters](https://python.langchain.com/docs/concepts/text_splitters/) break large Documents into smaller chunks. This is useful both for indexing data and for passing it in to a model, since large chunks are harder to search over and won't fit in a model's finite context window.\n",
    "3. Store: We need somewhere to store and index our splits, so that they can later be searched over. This is often done using a [VectorStore](https://python.langchain.com/docs/concepts/#vector-stores) and [Embeddings](https://python.langchain.com/docs/concepts/embedding_models/) model.\n",
    "\n",
    "![](RAG_VectorStore.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d8e9a2",
   "metadata": {},
   "source": [
    "**Retrieval and Generation**\n",
    "\n",
    "4. Retrieve: Given a user input, relevant splits are retrieved from storage using a [Retriever](https://python.langchain.com/docs/concepts/retrievers/).\n",
    "5. Generate: A ChatModel / LLM produces an answer using a prompt that includes the question and the retrieved data.\n",
    "\n",
    "![](Retrieve_Generate.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88555a-53a5-4ab8-ba3d-e6dd3a26c71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install langchain_community tiktoken langchain-openai langchainhub chromadb langchain sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098456a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "959397ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    " \n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eae0ab7-d43b-43e0-8b99-6122a636fe0c",
   "metadata": {},
   "source": [
    "## Part 1: Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff35dda5",
   "metadata": {},
   "source": [
    " \n",
    "[RAG quickstart](https://python.langchain.com/docs/tutorials/rag/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98070313-0c2f-4ba6-ae3e-79e2418ce4df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is the process of breaking down complex tasks into smaller, manageable steps. Techniques like Chain of Thought (CoT) and Tree of Thoughts facilitate this by prompting models to think step by step and explore multiple reasoning possibilities. It can be achieved through simple prompts, task-specific instructions, or human inputs.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "\n",
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "# Embed\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# Prompt\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Chain\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Question\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "00aa3f07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_v/9gq712613816czhrgk8ry0tw0000gp/T/ipykernel_11882/1462192070.py:17: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
      "/Users/nicyscaria/miniconda3/envs/auro/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MALCOLM. Let us seek out some desolate shade and there\n",
      "    Weep our sad bosoms empty.\n",
      "  MACDUFF. Let us rather\n",
      "    Hold fast the mortal sword, and like good men\n",
      "    Bestride our downfall'n birthdom. Each new morn\n",
      "    New widows howl, new orphans cry, new sorrows\n",
      "    Strike heaven on the face, that it resounds\n",
      "    As if it felt with Scotland and yell'd out\n",
      "    Like syllable of dolor.\n",
      "  MALCOLM. What I believe, I'll wall;\n",
      "    What know, believe; and what I can redress,\n",
      "    As I shall find the time to friend, I will.\n",
      "    What you have spoke, it may be so perchance.\n",
      "    This tyrant, whose sole name blisters our tongues,\n",
      "    Was once thought honest. You have loved him well;\n",
      "    He hath not touch'd you yet. I am young, but something\n",
      "    You may deserve of him through me, and wisdom  \n",
      "    To offer up a weak, poor, innocent lamb\n",
      "    To appease an angry god.\n",
      "  MACDUFF. I am not treacherous.\n",
      "  MALCOLM. But Macbeth is.\n",
      "    A good and virtuous nature may recoil\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "# load the document and split it into chunks\n",
    "loader = TextLoader(\"shakespeare.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "# split it into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=200)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# load it into Chroma with a specific collection name\n",
    "db = Chroma.from_documents(\n",
    "    docs, \n",
    "    embedding_function,\n",
    "    collection_name=\"shakespeare\"\n",
    ")\n",
    "\n",
    "# query it\n",
    "query = \"What is Malcolm?\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "76dca16a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The provided context does not mention \"Malcolm.\" Therefore, based on the information available, I cannot provide an answer regarding what Malcolm is. If you have a specific context or details about Malcolm, please share them, and I would be happy to help!' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 9380, 'total_tokens': 9431, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BdVrxxH9z3N3QYbbl0mAPocSs0cnZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--ab0d7a05-9a2f-4201-880a-3839af6d5820-0' usage_metadata={'input_tokens': 9380, 'output_tokens': 51, 'total_tokens': 9431, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "# Combine the content of the retrieved documents for context\n",
    "context = docs[0].page_content\n",
    "\n",
    "# Create a prompt for the language model\n",
    "prompt = PromptTemplate.from_template(\n",
    "    \"Based on the following context, answer the question:\\n\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "# Initialize the language model (e.g., OpenAI)\n",
    "llm_model=\"gpt-4o-mini\"\n",
    "llm = ChatOpenAI(temperature=0.0, model=llm_model) # Replace with your model of choice\n",
    "\n",
    "# Generate an answer using the language model\n",
    "answer = llm.invoke(prompt.format(context=context, query=query))\n",
    "\n",
    "# Print the generated answer\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e8e856-bafd-469e-b99a-11596b18aad4",
   "metadata": {},
   "source": [
    "## Part 2: Indexing\n",
    "\n",
    "There are different algorithms to index the documents/splits generated out of the initial document: Hierarchical Navigable Small World (HNSW - Chroma, Inverted File Index - FAISS and Pinecone, Locality Sensitive Hashing, Tree-Based Indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "edd7beeb-21fa-4f4b-b8fa-5a4f70489a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents\n",
    "question = \"What kinds of pets do I like?\"\n",
    "document = \"My favorite pet is a cat.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df119cca-1676-4caa-bad4-11805d69e616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def num_tokens_from_string(string: str, encoding_name: str) -> int:\n",
    "    encoding = tiktoken.get_encoding(encoding_name)\n",
    "    num_tokens = len(encoding.encode(string))\n",
    "    return num_tokens\n",
    "\n",
    "num_tokens_from_string(question, \"cl100k_base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f04fd74-829f-472c-a1bc-ec6521a0529f",
   "metadata": {},
   "source": [
    "There are different [embedding models](https://python.langchain.com/docs/integrations/text_embedding/) available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bd98786-755d-4d49-ba97-30c5a623b74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of query: 1536\n",
      "Length of document: 1536\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "embd = OpenAIEmbeddings()\n",
    "query_result = embd.embed_query(question)\n",
    "document_result = embd.embed_query(document)\n",
    "\n",
    "print(f'Length of query: {len(query_result)}')\n",
    "print(f'Length of document: {len(document_result)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8001998-b08c-4560-b124-bfa1fced8958",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity: 0.8806915835035409\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)\n",
    "\n",
    "similarity = cosine_similarity(query_result, document_result)\n",
    "print(\"Cosine Similarity:\", similarity)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aea73bc-98e3-4fdc-ba72-d190736bed20",
   "metadata": {},
   "source": [
    "[Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5778c31a-6138-4130-8865-31a08e82b9fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e731e-c6ff-46e3-a8bc-386832362af2",
   "metadata": {},
   "source": [
    "There are different [text splitters](https://python.langchain.com/docs/concepts/text_splitters/) available.\n",
    "\n",
    "`RecursiveCharacterTextSplitter` is the recommended one for generic text. It is parameterized by a list of characters. It tries to split on them in order until the chunks are small enough. The default list is [\"\\n\\n\", \"\\n\", \" \", \"\"]. This has the effect of trying to keep all paragraphs (and then sentences, and then words) together as long as possible, as those would generically seem to be the strongest semantically related pieces of text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e668d339-3951-4662-8387-c3d296646906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427303a1-3ed4-430c-bfc7-cb3e48022f1d",
   "metadata": {},
   "source": [
    "[Vectorstores](https://python.langchain.com/docs/integrations/vectorstores/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "baa90aaf-cc1b-46a1-9fba-cf20804dcb41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba890329-1411-4922-bd27-fe0490dd1208",
   "metadata": {},
   "source": [
    "## Part 3: Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fafdada1-4c4e-41f8-ad1a-33861aae3930",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=OpenAIEmbeddings())\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 3}) # picks the one with highest similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "57c2de7a-93e6-4072-bc5b-db6516f96dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = retriever.get_relevant_documents(\"What is Task Decomposition?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5a92d84c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "db96f877-60d3-4741-9846-e2903831583d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beda1b07-7bd2-4f5b-8d44-1fc52f5d2ce2",
   "metadata": {},
   "source": [
    "## Part 4: Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8beb6c14-5e18-43e7-9d04-59e3b8a81cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template='Answer the question based only on the following context:\\n{context}\\n\\nQuestion: {question}\\n'), additional_kwargs={})])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4461264-5cac-479a-917c-9bf589826da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LLM\n",
    "llm = ChatOpenAI(model_name=\"gpt-4o-mini\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "55d6629f-18ec-4372-a557-b254fbb1dd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain\n",
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "94470770-8df4-4359-9504-ef6c8b3137ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Task Decomposition is the process by which an agent breaks down large tasks into smaller, manageable subgoals. This enables the agent to handle complex tasks more efficiently. It can be achieved through various methods, such as prompting the LLM with specific questions about the steps needed to accomplish a task, using task-specific instructions, or incorporating human inputs. Task Decomposition is essential for enhancing the model's performance on complex tasks by transforming them into simpler, more manageable components.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 93, 'prompt_tokens': 9989, 'total_tokens': 10082, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BdVt3f3fW7TINcN25pMLJlyA2EJ7X', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--d550db3c-760e-48f4-943d-ef377781b272-0', usage_metadata={'input_tokens': 9989, 'output_tokens': 93, 'total_tokens': 10082, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run\n",
    "chain.invoke({\"context\":docs,\"question\":\"What is Task Decomposition?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "65770e2d-3d5e-4371-abc9-0aeca9646885",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f53e5840-0a0f-4428-a4a4-6922800aff89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, metadata={'lc_hub_owner': 'rlm', 'lc_hub_repo': 'rag-prompt', 'lc_hub_commit_hash': '50442af133e61576e74536c6556cefe1fac147cad032f4377b60c436e6cdcb6e'}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context', 'question'], input_types={}, partial_variables={}, template=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"), additional_kwargs={})])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_hub_rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8208a8bc-c75f-4e8e-8601-680746cd6276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task Decomposition is the process of breaking down a complicated task into smaller, more manageable steps. Techniques like Chain of Thought (CoT) and Tree of Thoughts enhance this process by guiding models to think step by step and explore multiple reasoning possibilities. This can be achieved through simple prompts, task-specific instructions, or human inputs.'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "    | prompt_hub_rag\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "rag_chain.invoke(\"What is Task Decomposition?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "auro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
